{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 Multiply Usage - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000000 should equal 2.0\n",
      "9.000000 should equal 9.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(\"float\") # Create a symbolic variable 'a'\n",
    "b = tf.placeholder(\"float\") # Create a symbolic variable 'b'\n",
    "\n",
    "y = tf.mul(a, b) # multiply the symbolic variables\n",
    "\n",
    "with tf.Session() as sess: # create a session to evaluate the symbolic expressions\n",
    "    print(\"%f should equal 2.0\" % sess.run(y, feed_dict={a: 1, b: 2})) # eval expressions with parameters for a and b\n",
    "    print(\"%f should equal 9.0\" % sess.run(y, feed_dict={a: 3, b: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.87943\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "trX = np.linspace(-1, 1, 101)\n",
    "trY = 2 * trX + np.random.randn(*trX.shape) * 0.33 # create a y value which is approximately linear but with some random noise\n",
    "\n",
    "X = tf.placeholder(\"float\") # create symbolic variables\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "def model(X, w):\n",
    "    return tf.mul(X, w) # lr is just X*w so this model line is pretty simple\n",
    "\n",
    "\n",
    "w = tf.Variable(0.0, name=\"weights\") # create a shared variable (like theano.shared) for the weight matrix\n",
    "y_model = model(X, w)\n",
    "\n",
    "cost = tf.square(Y - y_model) # use square error for cost function\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost) # construct an optimizer to minimize cost and fit line to my data\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize variables (in this case just variable W)\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(100):\n",
    "        for (x, y) in zip(trX, trY):\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "\n",
    "    print(sess.run(w))  # It should be something around 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', 'train-images-idx3-ubyte.gz', 9912422, 'bytes.')\n",
      "('Extracting', 'MNIST_data/train-images-idx3-ubyte.gz')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/gzip.py:268: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  chunk = self.extrabuf[offset: offset + size]\n",
      "input_data.py:42: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  data = data.reshape(num_images, rows, cols, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', 'train-labels-idx1-ubyte.gz', 28881, 'bytes.')\n",
      "('Extracting', 'MNIST_data/train-labels-idx1-ubyte.gz')\n",
      "('Succesfully downloaded', 't10k-images-idx3-ubyte.gz', 1648877, 'bytes.')\n",
      "('Extracting', 'MNIST_data/t10k-images-idx3-ubyte.gz')\n",
      "('Succesfully downloaded', 't10k-labels-idx1-ubyte.gz', 4542, 'bytes.')\n",
      "('Extracting', 'MNIST_data/t10k-labels-idx1-ubyte.gz')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "def model(X, w):\n",
    "    return tf.matmul(X, w) # notice we use the same model as linear regression, this is because there is a baked in cost function which performs softmax and cross entropy\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.8841)\n",
      "(1, 0.89580000000000004)\n",
      "(2, 0.90249999999999997)\n",
      "(3, 0.90649999999999997)\n",
      "(4, 0.90959999999999996)\n",
      "(5, 0.91080000000000005)\n",
      "(6, 0.91180000000000005)\n",
      "(7, 0.91339999999999999)\n",
      "(8, 0.91439999999999999)\n",
      "(9, 0.91539999999999999)\n",
      "(10, 0.91639999999999999)\n",
      "(11, 0.91700000000000004)\n",
      "(12, 0.91749999999999998)\n",
      "(13, 0.91749999999999998)\n",
      "(14, 0.91749999999999998)\n",
      "(15, 0.91800000000000004)\n",
      "(16, 0.91869999999999996)\n",
      "(17, 0.91869999999999996)\n",
      "(18, 0.91910000000000003)\n",
      "(19, 0.9194)\n",
      "(20, 0.91949999999999998)\n",
      "(21, 0.92000000000000004)\n",
      "(22, 0.92010000000000003)\n",
      "(23, 0.92020000000000002)\n",
      "(24, 0.9204)\n",
      "(25, 0.92079999999999995)\n",
      "(26, 0.92100000000000004)\n",
      "(27, 0.92100000000000004)\n",
      "(28, 0.92120000000000002)\n",
      "(29, 0.9214)\n",
      "(30, 0.92159999999999997)\n",
      "(31, 0.92169999999999996)\n",
      "(32, 0.92200000000000004)\n",
      "(33, 0.92179999999999995)\n",
      "(34, 0.92159999999999997)\n",
      "(35, 0.92149999999999999)\n",
      "(36, 0.92169999999999996)\n",
      "(37, 0.92169999999999996)\n",
      "(38, 0.92190000000000005)\n",
      "(39, 0.92190000000000005)\n",
      "(40, 0.92179999999999995)\n",
      "(41, 0.92220000000000002)\n",
      "(42, 0.92210000000000003)\n",
      "(43, 0.92210000000000003)\n",
      "(44, 0.92179999999999995)\n",
      "(45, 0.92200000000000004)\n",
      "(46, 0.92200000000000004)\n",
      "(47, 0.92210000000000003)\n",
      "(48, 0.92210000000000003)\n",
      "(49, 0.92230000000000001)\n",
      "(50, 0.92230000000000001)\n",
      "(51, 0.92220000000000002)\n",
      "(52, 0.9224)\n",
      "(53, 0.92249999999999999)\n",
      "(54, 0.92230000000000001)\n",
      "(55, 0.92269999999999996)\n",
      "(56, 0.92279999999999995)\n",
      "(57, 0.92290000000000005)\n",
      "(58, 0.92300000000000004)\n",
      "(59, 0.92310000000000003)\n",
      "(60, 0.92310000000000003)\n",
      "(61, 0.92320000000000002)\n",
      "(62, 0.9234)\n",
      "(63, 0.92349999999999999)\n",
      "(64, 0.9234)\n",
      "(65, 0.92349999999999999)\n",
      "(66, 0.92359999999999998)\n",
      "(67, 0.92369999999999997)\n",
      "(68, 0.92369999999999997)\n",
      "(69, 0.92379999999999995)\n",
      "(70, 0.92379999999999995)\n",
      "(71, 0.92379999999999995)\n",
      "(72, 0.92390000000000005)\n",
      "(73, 0.92390000000000005)\n",
      "(74, 0.92390000000000005)\n",
      "(75, 0.92400000000000004)\n",
      "(76, 0.92390000000000005)\n",
      "(77, 0.92390000000000005)\n",
      "(78, 0.92390000000000005)\n",
      "(79, 0.92390000000000005)\n",
      "(80, 0.92379999999999995)\n",
      "(81, 0.92369999999999997)\n",
      "(82, 0.92379999999999995)\n",
      "(83, 0.92390000000000005)\n",
      "(84, 0.92400000000000004)\n",
      "(85, 0.92390000000000005)\n",
      "(86, 0.92390000000000005)\n",
      "(87, 0.92379999999999995)\n",
      "(88, 0.92379999999999995)\n",
      "(89, 0.92369999999999997)\n",
      "(90, 0.92379999999999995)\n",
      "(91, 0.92379999999999995)\n",
      "(92, 0.92369999999999997)\n",
      "(93, 0.92369999999999997)\n",
      "(94, 0.92369999999999997)\n",
      "(95, 0.92359999999999998)\n",
      "(96, 0.92349999999999999)\n",
      "(97, 0.92359999999999998)\n",
      "(98, 0.92349999999999999)\n",
      "(99, 0.92359999999999998)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784]) # create symbolic variables\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "w = init_weights([784, 10]) # like in linear regression, we need a shared variable weight matrix for logistic regression\n",
    "\n",
    "py_x = model(X, w)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) # compute mean cross entropy (softmax is applied internally)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct optimizer\n",
    "predict_op = tf.argmax(py_x, 1) # at predict time, evaluate the argmax of the logistic regression\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(100):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "        print(i, np.mean(np.argmax(teY, axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX, Y: teY})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspired by https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "batch_size = 20\n",
    "# Dimension of the embedding vector. Two too small to get\n",
    "# any meaningful embeddings, but let's make it 2 for simple visualization\n",
    "embedding_size = 2\n",
    "num_sampled = 15    # Number of negative examples to sample.\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"the quick brown fox jumped over the lazy dog\",\n",
    "            \"I love cats and dogs\",\n",
    "            \"we all love cats and dogs\",\n",
    "            \"cats and dogs are great\",\n",
    "            \"sung likes cats\",\n",
    "            \"she loves dogs\",\n",
    "            \"cats can be very independent\",\n",
    "            \"cats are great companions when they want to be\",\n",
    "            \"cats are playful\",\n",
    "            \"cats are natural hunters\",\n",
    "            \"It's raining cats and dogs\",\n",
    "            \"dogs and cats love sung\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Word count', [('cats', 10), ('dogs', 6), ('and', 5), ('are', 4), ('love', 3)])\n"
     ]
    }
   ],
   "source": [
    "# sentences to words and count\n",
    "words = \" \".join(sentences).split()\n",
    "count = collections.Counter(words).most_common()\n",
    "print (\"Word count\", count[:5])\n",
    "\n",
    "# Build dictionaries\n",
    "rdic = [i[0] for i in count] #reverse dic, idx -> word\n",
    "dic = {w: i for i, w in enumerate(rdic)} #dic, word -> id\n",
    "voc_size = len(dic)\n",
    "\n",
    "# Make indexed word data\n",
    "data = [dic[word] for word in words]\n",
    "print('Sample data', data[:10], [rdic[t] for t in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Context pairs', [[[8, 24], 33], [[33, 20], 24], [[24, 17], 20], [[20, 12], 17], [[17, 8], 12], [[12, 25], 8], [[8, 30], 25], [[25, 26], 30], [[30, 4], 26], [[26, 0], 4]])\n"
     ]
    }
   ],
   "source": [
    "# Let's make a training data for window size 1 for simplicity\n",
    "# ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...\n",
    "cbow_pairs = [];\n",
    "for i in range(1, len(data)-1) :\n",
    "    cbow_pairs.append([[data[i-1], data[i+1]], data[i]]);\n",
    "print('Context pairs', cbow_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('skip-gram pairs', [[33, 8], [33, 24], [24, 33], [24, 20], [20, 24]])\n"
     ]
    }
   ],
   "source": [
    "# Let's make skip-gram pairs\n",
    "# (quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
    "skip_gram_pairs = [];\n",
    "for c in cbow_pairs:\n",
    "    skip_gram_pairs.append([c[1], c[0][0]])\n",
    "    skip_gram_pairs.append([c[1], c[0][1]])\n",
    "print('skip-gram pairs', skip_gram_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Batches (x, y)', ([32, 14, 25], [[1], [3], [8]]))\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(size):\n",
    "    assert size < len(skip_gram_pairs)\n",
    "    x_data=[]\n",
    "    y_data = []\n",
    "    r = np.random.choice(range(len(skip_gram_pairs)), size, replace=False)\n",
    "    for i in r:\n",
    "        x_data.append(skip_gram_pairs[i][0])  # n dim\n",
    "        y_data.append([skip_gram_pairs[i][1]])  # n, 1 dim\n",
    "    return x_data, y_data\n",
    "\n",
    "# generate_batch test\n",
    "print ('Batches (x, y)', generate_batch(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# need to shape [batch_size, 1] for nn.nce_loss\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs) # lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss at ', 0, 16.493217)\n",
      "('Loss at ', 10, 8.7124729)\n",
      "('Loss at ', 20, 5.2837801)\n",
      "('Loss at ', 30, 3.7116096)\n",
      "('Loss at ', 40, 4.084024)\n",
      "('Loss at ', 50, 3.8839774)\n",
      "('Loss at ', 60, 3.7806091)\n",
      "('Loss at ', 70, 3.4853051)\n",
      "('Loss at ', 80, 3.542469)\n",
      "('Loss at ', 90, 3.4044869)\n"
     ]
    }
   ],
   "source": [
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.random_uniform([voc_size, embedding_size],-1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch.\n",
    "# This does the magic:\n",
    "#   tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes ...)\n",
    "# It automatically draws negative samples when we evaluate the loss.\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                 num_sampled, voc_size))\n",
    "\n",
    "# Use the adam optimizer\n",
    "train_op = tf.train.AdamOptimizer(1e-1).minimize(loss)\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # Initializing all variables\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for step in range(100):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size)\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                feed_dict={train_inputs: batch_inputs, train_labels: batch_labels})\n",
    "        if step % 10 == 0:\n",
    "          print(\"Loss at \", step, loss_val) # Report the loss\n",
    "\n",
    "    # Final embeddings are ready for you to use. Need to normalize for practical use\n",
    "    trained_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48fb7e0d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show word2vec if dim is 2\n",
    "if trained_embeddings.shape[1] == 2:\n",
    "    labels = rdic[:10] # Show top 10 words\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = trained_embeddings[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
    "            textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.savefig(\"word2vec.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
